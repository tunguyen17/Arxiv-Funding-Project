{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>\n",
    "Optimal Academic Funding\n",
    "</h1>\n",
    "\n",
    "<h3>\n",
    "Tu Anh Nguyen\n",
    "</h3>\n",
    "<h4>\n",
    "Tarleton State University, Stephenville, TX\n",
    "</h4>\n",
    "<h4>\n",
    "12/04/2017\n",
    "</h4>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This project is a preliminarily research on exploring effect of varying funding levels. Our goal is to find an optimal distribution of funding so that the number of paper produced each year are optimized. Please note that, this research only focuses on academic papers that are in Science, technology, engineering, and mathematics (STEM). We hope to expand our reseach into other disciplin to have a broader picture.\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "The data we use for this project is collected fron arXiv (https://arxiv.org/). The arXiv is a repository that contains papers from Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering and Systems Science, and Economics. For each paper in arXiv, we collect the following data,\n",
    "\n",
    "1. doi\n",
    "1. Publishing date\n",
    "1. Title\n",
    "1. Author names\n",
    "1. Main Field\n",
    "\n",
    "According to https://arxiv.org/help/bulk_data, there are 3 services that we can use for colleting the data from arXiv. They are `OAI-PMH`, `arXiv API`, and `RSS`. Our fist approach was using the arXiv. However, there are inconsistency in the category values because of the recent changes in the naming system for category. Furthermore, `arXiv API` was not desgined for accessing the whole arXiv repository. Thus, we use `OAI-PMH` to acces the metadata for every paper in the arXiv repository. \n",
    "\n",
    "### Data Collection Process\n",
    "\n",
    "We use the `urllib` to request a `url` link that serves as the query interface for arXiv's `OAI-PMH`. After the request, arXiv's server will return a text strings in `XML` format that contains the results for the report. We use the `BeautifulSoup` package to parse the data. Each query returns at most 1,000 results. If there are more than, 1,000 results, arXiv will provide a `resumptionToken` that can be used for continue to the next 1,000 results. The python code for the data collection process is provided in the appendix. The data collected are exported into a csv file, which can be acceses at https://www.dropbox.com/s/mxlqmphe9dxtw8y/data_v3.csv?dl=1. Please note that the size of this files is (371M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "In order to process the data, we need to import important packages. \n",
    "\n",
    "1. We use `pandas` for parsing csv files and organizing data\n",
    "1. We use `numpy` as our advance calculation tool\n",
    "1. we use `matplotlib.pyplot` for the visualization of our result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is too big for github. We have to host our data using dropbox. Once, we have downloaded the data to our local machine, we can change `download` to `False` in order to avoid having do download the data gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download and import data\n",
    "download = True # If download is true download and save data, else just read data\n",
    "\n",
    "if(download):\n",
    "    ## Data set\n",
    "    data_url = \"https://www.dropbox.com/s/mxlqmphe9dxtw8y/data_v3.csv?dl=1\"\n",
    "    df = pd.read_csv(data_url)\n",
    "    df.to_csv(\"data.csv\", index = False)\n",
    "else:\n",
    "    df = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there might be duplications and `NaN` in the dataset, we uses pandas `dropna` and `drop_duplicates` to remove them from our data frame. Furthermore, we also use `drop` to remove the index value from the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oai:arXiv.org:0704.0002</td>\n",
       "      <td>2007-03-30</td>\n",
       "      <td>Sparsity-certifying Graph Decompositions</td>\n",
       "      <td>Streinu Ileana;Theran Louis</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oai:arXiv.org:0704.0046</td>\n",
       "      <td>2007-04-01</td>\n",
       "      <td>A limit relation for entropy and channel capac...</td>\n",
       "      <td>Csiszar I.;Hiai F.;Petz D.</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oai:arXiv.org:0704.0047</td>\n",
       "      <td>2007-04-01</td>\n",
       "      <td>Intelligent location of simultaneously active ...</td>\n",
       "      <td>Kosel T.;Grabec I.</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oai:arXiv.org:0704.0050</td>\n",
       "      <td>2007-04-01</td>\n",
       "      <td>Intelligent location of simultaneously active ...</td>\n",
       "      <td>Kosel T.;Grabec I.</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oai:arXiv.org:0704.0062</td>\n",
       "      <td>2007-03-31</td>\n",
       "      <td>On-line Viterbi Algorithm and Its Relationship...</td>\n",
       "      <td>Šrámek Rastislav;Brejová Broňa;Vinař Tomáš</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       doi        date  \\\n",
       "0  oai:arXiv.org:0704.0002  2007-03-30   \n",
       "1  oai:arXiv.org:0704.0046  2007-04-01   \n",
       "2  oai:arXiv.org:0704.0047  2007-04-01   \n",
       "3  oai:arXiv.org:0704.0050  2007-04-01   \n",
       "4  oai:arXiv.org:0704.0062  2007-03-31   \n",
       "\n",
       "                                               title  \\\n",
       "0           Sparsity-certifying Graph Decompositions   \n",
       "1  A limit relation for entropy and channel capac...   \n",
       "2  Intelligent location of simultaneously active ...   \n",
       "3  Intelligent location of simultaneously active ...   \n",
       "4  On-line Viterbi Algorithm and Its Relationship...   \n",
       "\n",
       "                                      authors category  \n",
       "0                 Streinu Ileana;Theran Louis       cs  \n",
       "1                  Csiszar I.;Hiai F.;Petz D.       cs  \n",
       "2                          Kosel T.;Grabec I.       cs  \n",
       "3                          Kosel T.;Grabec I.       cs  \n",
       "4  Šrámek Rastislav;Brejová Broňa;Vinař Tomáš       cs  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning the dataframe\n",
    "df.dropna(inplace = True)\n",
    "df.drop_duplicates(subset = \"doi\", inplace = True)\n",
    "df.drop(\"Unnamed: 0\", axis = 1, inplace = True) # Drop the \"Unnamed: 0\"\n",
    "\n",
    "# Get the category list\n",
    "all_cat = list(set(df[\"category\"].values))\n",
    "all_cat.sort()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collecting all the aurhors\n",
    "au_lst = []\n",
    "for paper_authors in df[\"authors\"].values:\n",
    "    for author in paper_authors.split(\";\"):\n",
    "        au_lst.append(author)\n",
    "        \n",
    "# Get all the unique authors       \n",
    "au_lst = list(set(au_lst))\n",
    "au_lst.sort()\n",
    "\n",
    "au_dict = {author:index for (index, author) in enumerate(au_lst)}\n",
    "cat_dict = {cat:index for (index, cat) in enumerate(all_cat)}\n",
    "\n",
    "# Creating the matrix\n",
    "n = len(au_dict)\n",
    "p = len(all_cat)\n",
    "credit_matrix = np.zeros((n, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for index, row in df[[\"authors\", \"category\"]].iterrows():\n",
    "    \n",
    "    author_list = row[\"authors\"].split(\";\")\n",
    "    contribute = 1.0/len(author_list)\n",
    "    \n",
    "    for author in author_list:\n",
    "        try:\n",
    "            credit_matrix[ au_dict[author], cat_dict[row[\"category\"]] ] += contribute\n",
    "        except KeyError as e:\n",
    "            print(e)\n",
    "\n",
    "# Calculating stuff\n",
    "author_activity = credit_matrix / credit_matrix.sum(axis=1, keepdims=True)\n",
    "author_weight_in_field = credit_matrix / credit_matrix.sum(axis=0, keepdims=True)\n",
    "field_field_influence = np.transpose(author_activity).dot(author_weight_in_field)\n",
    "\n",
    "proj1_df = pd.DataFrame(field_field_influence, columns = all_cat, index=all_cat)\n",
    "proj1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credit_df = pd.DataFrame(credit_matrix, columns = all_cat, index = au_lst)\n",
    "credit_df.to_csv(\"credit_data.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "credit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_author_funding(credit, field_funding):\n",
    "    author_weight_in_field = credit / credit.sum(axis=0,keepdims=True)\n",
    "    author_funding_from_field = author_weight_in_field * field_funding\n",
    "    author_funding = author_funding_from_field.sum(axis=1,keepdims=True)\n",
    "    return author_funding\n",
    "\n",
    "def compute_credit(author_funding):\n",
    "    new_credit = author_prod * author_funding\n",
    "    field_credit = new_credit.sum(axis=0)\n",
    "    author_credit = new_credit.sum(axis=1)\n",
    "    total_credit = new_credit.sum()\n",
    "    return new_credit, total_credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_field = len(all_cat)\n",
    "num_auth  = len(au_lst)\n",
    "num_steps = 200\n",
    "\n",
    "# Learning hyperparameter\n",
    "p = 0.05\n",
    "alpha = 0.1\n",
    "\n",
    "# Current credit\n",
    "current_credit = credit_matrix\n",
    "\n",
    "# Current field funding - Generating a random funding\n",
    "d = np.random.rand(num_field)\n",
    "current_field_funding = d / d.sum()\n",
    "\n",
    "# Saving the original field funding \n",
    "original_field_funding = current_field_funding.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initial calculation\n",
    "current_author_funding = update_author_funding(current_credit, current_field_funding)\n",
    "author_prod = current_credit / current_author_funding # This is invariance\n",
    "current_credit, current_total_credit = compute_credit(current_author_funding)\n",
    "\n",
    "# Initialize the best state\n",
    "best_field_funding = current_field_funding.copy()\n",
    "best_credit        = current_credit.copy()\n",
    "best_total_credit  = current_total_credit.copy()\n",
    "\n",
    "tot_credit_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_steps):\n",
    "    current_author_weight_in_field = current_credit / current_credit.sum(axis=0,keepdims=True)\n",
    "\n",
    "    if(np.random.rand() < 0.05):\n",
    "        gradient = np.random.rand(num_field)\n",
    "    else:\n",
    "        gradient = (author_prod * current_author_weight_in_field).sum(axis = 0)\n",
    "\n",
    "    gradient_norm = gradient/(sum(gradient))  # normalize\n",
    "    # Update field funding\n",
    "    new_field_funding = current_field_funding + alpha*gradient_norm\n",
    "    new_field_funding = new_field_funding / (sum(new_field_funding)) # normalize \n",
    "\n",
    "    new_author_funding = update_author_funding(current_credit, new_field_funding)\n",
    "    new_credit, new_total_credit = compute_credit(new_author_funding)\n",
    "    \n",
    "    tot_credit_lst.append(new_total_credit)\n",
    "    \n",
    "    # update the new best result\n",
    "    if(best_total_credit < new_total_credit):\n",
    "        best_field_funding = new_field_funding.copy()\n",
    "        best_credit        = new_credit.copy()\n",
    "        best_total_credit  = new_total_credit.copy()\n",
    "\n",
    "#         print(\"new Best\")\n",
    "\n",
    "    # Update for new step \n",
    "    current_field_funding = new_field_funding.copy()\n",
    "    current_credit        = new_credit.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(tot_credit_lst)\n",
    "plt.ylabel('Total Credit')\n",
    "plt.xlabel('step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "funding_df = pd.DataFrame(best_field_funding, columns = [\"Field Funding\"], index = all_cat)\n",
    "funding_df.sort_values(by=\"Field Funding\", ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is the python code used for scraping the data from the arXiv repository using `OAI-PMH`.\n",
    "```python\n",
    "# Import for processing XML\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Import for requesting HTML\n",
    "import urllib\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "# Import for text processing\n",
    "import io\n",
    "import re\n",
    "\n",
    "# Import for data processing and organization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Find all the categories\n",
    "url = \"http://export.arxiv.org/oai2?verb=ListSets\"\n",
    "u = urllib.request.urlopen(url, data = None)\n",
    "f = io.TextIOWrapper(u,encoding='utf-8')\n",
    "text = f.read()\n",
    "soup = BeautifulSoup(text, 'xml')\n",
    "all_cat = [sp.text for sp in soup.findAll(\"setSpec\")]\n",
    "\n",
    "# Export the categories to a txt files\n",
    "f = open(\"all_cat_v01.txt\", \"w\")\n",
    "f.write(\",\".join(all_cat))\n",
    "f.close()\n",
    "\n",
    "def scrape(cat):\n",
    "\n",
    "    '''\n",
    "    Function to scrape all the paper from a category\n",
    "\n",
    "    INPUT : category (String)\n",
    "    OUTPUT: dataframe that contains doi, date, title, authors, category for each paper (pandas.DataFrame)\n",
    "    '''\n",
    "\n",
    "    # Initialization\n",
    "    df = pd.DataFrame(columns=(\"doi\", \"date\", \"title\", \"authors\", \"category\"))\n",
    "    base_url = \"http://export.arxiv.org/oai2?verb=ListRecords&\"\n",
    "    url = base_url + \"set={}&metadataPrefix=arXiv\".format(cat)\n",
    "\n",
    "    # while loop in order to loop through all the resutls\n",
    "    while True:\n",
    "        # print url to keep track of the progress\n",
    "        print(url)\n",
    "        # accessing the url\n",
    "        try:\n",
    "            u = urllib.request.urlopen(url, data = None)\n",
    "        except HTTPError as e:\n",
    "            # Incase of some error that require us to wait\n",
    "            if e.code == 503:\n",
    "                to = int(e.hdrs.get(\"retry-after\", 30))\n",
    "                print(\"Got 503. Retrying after {0:d} seconds.\".format(to))\n",
    "                time.sleep(to)\n",
    "                continue # Skip this loop, continue to the next one\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # read the request\n",
    "        f = io.TextIOWrapper(u,encoding='utf-8')\n",
    "        text = f.read()\n",
    "        soup = BeautifulSoup(text, 'xml')\n",
    "\n",
    "        # collect the data\n",
    "        for record in soup.findAll(\"record\"):\n",
    "            try:\n",
    "                doi = record.find(\"identifier\").text\n",
    "            except:\n",
    "                doi = np.nan\n",
    "\n",
    "            try:\n",
    "                date = record.find(\"created\").text\n",
    "            except:\n",
    "                date = np.nan\n",
    "\n",
    "            try:\n",
    "                title = record.find(\"title\").text\n",
    "            except:\n",
    "                title = np.nan\n",
    "\n",
    "            try:\n",
    "                authors = \";\".join([author.get_text(\" \") for author in record.findAll(\"author\")])\n",
    "            except:\n",
    "                authros = np.nan\n",
    "\n",
    "            try:\n",
    "                category = record.find(\"setSpec\").text\n",
    "            except:\n",
    "                category = np.nan\n",
    "\n",
    "            df = df.append({\"doi\":doi, \"date\":date, \"title\":title, \"authors\":authors, \"category\":category},\\\n",
    "                           ignore_index=True)\n",
    "\n",
    "\n",
    "        # resumptionToken help to find if there are more results\n",
    "        token = soup.find(\"resumptionToken\")\n",
    "        if token is None or token.text is None:\n",
    "            break\n",
    "        else:\n",
    "            url = base_url + \"resumptionToken=%s\"%(token.text)\n",
    "\n",
    "    return(df)\n",
    "\n",
    "# Initialize master_df that contains all the data points\n",
    "master_df = pd.DataFrame(columns=(\"doi\", \"date\", \"title\", \"authors\", \"category\"))\n",
    "for i in all_cat:\n",
    "    # Print out the current category for progress tracking\n",
    "    print(\"----------------\",i,\"-------------------\")\n",
    "    df = scrape(i)\n",
    "    # append the new data to master_df\n",
    "    master_df = master_df.append(df, ignore_index = True)\n",
    "\n",
    "# Export the data to a csv file\n",
    "master_df.to_csv(\"data.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
